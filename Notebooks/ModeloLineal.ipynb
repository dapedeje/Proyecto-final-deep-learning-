{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "336b7f3a",
      "metadata": {
        "id": "336b7f3a"
      },
      "source": [
        "<font color=\"FF3B3B\"><h1 align=\"left\">Proyecto de aprendizaje profundo</h1></font>\n",
        "<font color=\"#6E6E6E\"><h2 align=\"left\">Modelo lineal simple</h2></font>\n",
        "\n",
        "##### David Alejandro Pedroza De Jesús\n",
        "##### Joel Ezequiel Vicente Alonso\n",
        "\n",
        "Cargamos las librerias para la realización del proyecto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1db3fc64",
      "metadata": {
        "id": "1db3fc64"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from skimage.feature import hog\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "import os\n",
        "import kagglehub\n",
        "from shutil import move\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descarga, carga y limpieza de datos.\n",
        "\n",
        "En este paso cargamos los datos como lo hicimos en el analisis exploratorio"
      ],
      "metadata": {
        "id": "lleS49RFx5yg"
      },
      "id": "lleS49RFx5yg"
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"gpiosenka/cards-image-datasetclassification\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlLlbROKyIGk",
        "outputId": "74ec50a3-3115-42d9-a8bb-1c4f75d019a0"
      },
      "id": "HlLlbROKyIGk",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/gpiosenka/cards-image-datasetclassification?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 385M/385M [00:14<00:00, 28.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "move(path,os.getcwd())"
      ],
      "metadata": {
        "id": "BJJJGGSkx-Np",
        "outputId": "2c5ed6f8-5f94-4edc-8d37-4cf0ba00bddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "id": "BJJJGGSkx-Np",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "os.rename(\"2\", \"cards-image-datasetclassification\")"
      ],
      "metadata": {
        "id": "2BpolOyjyBIx"
      },
      "id": "2BpolOyjyBIx",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos las rutas que vamos a utilizar, las limpiaremos y adaptaremos."
      ],
      "metadata": {
        "id": "XtMj3oVwyXo0"
      },
      "id": "XtMj3oVwyXo0"
    },
    {
      "cell_type": "code",
      "source": [
        "datos = pd.read_csv(r\"cards-image-datasetclassification/cards.csv\")\n",
        "datos.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "U5nTtZ3Yyk_W",
        "outputId": "b5d17b2e-812f-4ff8-c809-74eb7fa05df4"
      },
      "id": "U5nTtZ3Yyk_W",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'cards-image-datasetclassification/cards.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3603/3587054979.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"cards-image-datasetclassification/cards.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdatos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cards-image-datasetclassification/cards.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quitaremos la ruta que no funciona y creamos un nuevo df con las rutas adaptadas y funcionales."
      ],
      "metadata": {
        "id": "MqPYmo9yy3ZV"
      },
      "id": "MqPYmo9yy3ZV"
    },
    {
      "cell_type": "code",
      "source": [
        "rutas_adaptadas = [\"cards-image-datasetclassification/\" +  ruta for ruta in datos[\"filepaths\"]]\n",
        "im_buenas = []\n",
        "im_malas = []\n",
        "for ru in rutas_adaptadas:\n",
        "    im = cv2.imread(ru)\n",
        "    if  im is not None:\n",
        "        im_buenas.append(ru)\n",
        "    else:\n",
        "        im_malas.append(ru)\n",
        "print(f\"Lecturas correctas: {len(im_buenas)}\")\n",
        "print(f\"Lecturas incorrectas: {len(im_malas)}\")\n",
        "\n",
        "df_limpio = datos[np.array(rutas_adaptadas) != np.array(im_malas)]\n",
        "df_limpio[\"rutas_adap\"] = im_buenas"
      ],
      "metadata": {
        "id": "ZiX2eJbZzHkU"
      },
      "id": "ZiX2eJbZzHkU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extración de caracteristicas.\n",
        "\n",
        "Para esta tarea podría emplearse una red neuronal para obtener un vector de características (embedding). Sin embargo, en este notebook evitaremos este tipo de aproximaciones, ya que no constituyen el objetivo principal del trabajo.\n",
        "\n",
        "En su lugar, utilizaremos HOG (Histogram of Oriented Gradients), un descriptor clásico de visión por computador basado en la distribución de gradientes de intensidad. Este método permite capturar información estructural y de contorno, representando eficazmente las formas presentes en la imagen.\n",
        "\n",
        "Este enfoque resulta especialmente adecuado en el caso de cartas, ya que contienen símbolos y figuras con geometrías bien definidas y altamente distintivas. HOG es particularmente eficaz para modelar este tipo de patrones.\n",
        "\n",
        "Una limitación del descriptor es que opera sobre imágenes en escala de grises, por lo que no aprovecha directamente la información cromática. No obstante, dado que la discriminación entre cartas depende en gran medida de la forma y estructura de los símbolos, es razonable esperar que esta representación sea suficiente para obtener un buen desempeño en la clasificación.\n",
        "\n",
        "Para implentar esto crearemos una función en python que pueda calcular el vector de cada imagen."
      ],
      "metadata": {
        "id": "wbYxUyPCzjws"
      },
      "id": "wbYxUyPCzjws"
    },
    {
      "cell_type": "code",
      "source": [
        "def features_extract(path):\n",
        "  img = cv2.imread(path)\n",
        "  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  features = hog(img_gray,\n",
        "                 pixels_per_cell=(8,8),\n",
        "                 cells_per_block=(2,2),\n",
        "                 feature_vector=True)\n",
        "  return features"
      ],
      "metadata": {
        "id": "OdPpP4h800__"
      },
      "id": "OdPpP4h800__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora obtenemos los vectores de caracteristicas de cada una de las imagenes."
      ],
      "metadata": {
        "id": "jJ27VolY1TqN"
      },
      "id": "jJ27VolY1TqN"
    },
    {
      "cell_type": "code",
      "source": [
        "rutas_train = df_limpio[df_limpio[\"data set\"] == \"train\"].rutas_adap\n",
        "rutas_test = df_limpio[df_limpio[\"data set\"] == \"test\"].rutas_adap\n",
        "rutas_val = df_limpio[df_limpio[\"data set\"] == \"valid\"].rutas_adap\n",
        "\n",
        "X_train = np.array([features_extract(p) for p in rutas_train])\n",
        "X_test = np.array([features_extract(p) for p in rutas_test])\n",
        "X_val = np.array([features_extract(p) for p in rutas_val])"
      ],
      "metadata": {
        "id": "JkeFse6_1bMF"
      },
      "id": "JkeFse6_1bMF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = df_limpio[df_limpio[\"data set\"] == \"train\"][\"labels\"]\n",
        "y_test = df_limpio[df_limpio[\"data set\"] == \"test\"][\"labels\"]\n",
        "y_val = df_limpio[df_limpio[\"data set\"] == \"valid\"][\"labels\"]"
      ],
      "metadata": {
        "id": "3oYs2SqF3_Wx"
      },
      "id": "3oYs2SqF3_Wx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparación de los datos\n",
        "\n",
        "## Normalización de los datos\n",
        "\n",
        "Siempre es necesario normalizar nuestros datos a la hora de entrenar modelo, en nuestro caso usamos `StandardScaler`."
      ],
      "metadata": {
        "id": "TCpWnTzs3P3d"
      },
      "id": "TCpWnTzs3P3d"
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_norm = scaler.fit_transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "X_val_norm = scaler.transform(X_val)"
      ],
      "metadata": {
        "id": "ozc8F2LN4oOl"
      },
      "id": "ozc8F2LN4oOl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Codificación de las etiquetas\n",
        "\n",
        "Usaremos label encoder para que podamos utilizar la regresión logistica, para esta tarea."
      ],
      "metadata": {
        "id": "CdoK6ZNK5EZm"
      },
      "id": "CdoK6ZNK5EZm"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y_train_enc = encoder.fit_transform(y_train.values.reshape(-1,1))\n",
        "y_test_enc = encoder.transform(y_test.values.reshape(-1,1))\n",
        "y_val_enc = encoder.transform(y_val.values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "a5HavTUj5LVz"
      },
      "id": "a5HavTUj5LVz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento y metricas.\n",
        "\n",
        "La regresión logistica y sklearnd en general no funciona como keras, por lo que si queremos tener la curva de validación y entrenamiento debemos de hacerlo a mano."
      ],
      "metadata": {
        "id": "v_hcQxR85lsO"
      },
      "id": "v_hcQxR85lsO"
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(\n",
        "    penalty=\"l2\",\n",
        "    solver=\"lbfgs\",\n",
        "    multi_class=\"multinomial\",\n",
        "    max_iter=2000\n",
        ")"
      ],
      "metadata": {
        "id": "Y1C-6H7P6edO"
      },
      "id": "Y1C-6H7P6edO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.fit(X_train_norm, y_train_enc)\n",
        "\n",
        "y_train_prob = model.predict_proba(X_train_norm)\n",
        "y_val_prob = model.predict_proba(X_val_norm)\n",
        "y_test_prob = model.predict_proba(X_test_norm)\n",
        "\n",
        "train_loss = log_loss(y_train_enc, y_train_prob)\n",
        "val_loss = log_loss(y_val_enc, y_val_prob)\n",
        "test_loss = log_loss(y_test_enc, y_test_prob)\n",
        "\n",
        "score = model.score(X_val_norm, y_val_enc)\n",
        "\n",
        "print(f\"Score: {score}\")\n",
        "print(f\"Train loss: {train_loss}\")\n",
        "print(f\"Val loss: {val_loss}\")\n",
        "print(f\"test loss: {test_loss}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_enc, model.predict(X_test_norm))}\")\n"
      ],
      "metadata": {
        "id": "I3_het_LGMDL"
      },
      "id": "I3_het_LGMDL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matriz de confusión\n",
        "\n",
        "Aunque no se pida al ser un problema de clasificación es importante mostrar su matriz de confución."
      ],
      "metadata": {
        "id": "jK1eYQ9EMKVu"
      },
      "id": "jK1eYQ9EMKVu"
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = encoder.classes_\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test_enc, model.predict(X_test_norm), normalize=\"true\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "sns.heatmap(cm,\n",
        "            annot=True, cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names,\n",
        "            cbar=True)\n",
        "\n",
        "plt.xlabel('Predicción')\n",
        "plt.ylabel('Verdadero')\n",
        "plt.title('Matriz de Confusión Tuneda')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K478vI3fMYbY"
      },
      "id": "K478vI3fMYbY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión\n",
        "\n",
        "Parece ser que el modelo de regresión logistica es una buena opción para tratar el problema, aunque como hemos visto en otros modelo aplicados a este problema tiene mucho margen de mejora."
      ],
      "metadata": {
        "id": "MpBJ64zFPM8w"
      },
      "id": "MpBJ64zFPM8w"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}