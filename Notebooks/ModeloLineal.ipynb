{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "336b7f3a",
      "metadata": {
        "id": "336b7f3a"
      },
      "source": [
        "<font color=\"FF3B3B\"><h1 align=\"left\">Proyecto de aprendizaje profundo</h1></font>\n",
        "<font color=\"#6E6E6E\"><h2 align=\"left\">Modelo lineal simple</h2></font>\n",
        "\n",
        "##### David Alejandro Pedroza De Jesús\n",
        "##### Joel Ezequiel Vicente Alonso\n",
        "\n",
        "Cargamos las librerias para la realización del proyecto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1db3fc64",
      "metadata": {
        "id": "1db3fc64"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from skimage.feature import hog\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "import os\n",
        "import kagglehub\n",
        "from shutil import move\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descarga, carga y limpieza de datos.\n",
        "\n",
        "En este paso cargamos los datos como lo hicimos en el analisis exploratorio"
      ],
      "metadata": {
        "id": "lleS49RFx5yg"
      },
      "id": "lleS49RFx5yg"
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"gpiosenka/cards-image-datasetclassification\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlLlbROKyIGk",
        "outputId": "79d6ca87-be08-46c1-fa3a-c61ac9081a4c"
      },
      "id": "HlLlbROKyIGk",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'cards-image-datasetclassification' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos las rutas que vamos a utilizar, las limpiaremos y adaptaremos."
      ],
      "metadata": {
        "id": "XtMj3oVwyXo0"
      },
      "id": "XtMj3oVwyXo0"
    },
    {
      "cell_type": "code",
      "source": [
        "datos = pd.read_csv(r\"cards-image-datasetclassification/cards.csv\")\n",
        "datos.head()"
      ],
      "metadata": {
        "id": "U5nTtZ3Yyk_W"
      },
      "id": "U5nTtZ3Yyk_W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quitaremos la ruta que no funciona y creamos un nuevo df con las rutas adaptadas y funcionales."
      ],
      "metadata": {
        "id": "MqPYmo9yy3ZV"
      },
      "id": "MqPYmo9yy3ZV"
    },
    {
      "cell_type": "code",
      "source": [
        "rutas_adaptadas = [\"cards-image-datasetclassification/\" +  ruta for ruta in datos[\"filepaths\"]]\n",
        "im_buenas = []\n",
        "im_malas = []\n",
        "for ru in rutas_adaptadas:\n",
        "    im = cv2.imread(ru)\n",
        "    if  im is not None:\n",
        "        im_buenas.append(ru)\n",
        "    else:\n",
        "        im_malas.append(ru)\n",
        "print(f\"Lecturas correctas: {len(im_buenas)}\")\n",
        "print(f\"Lecturas incorrectas: {len(im_malas)}\")\n",
        "\n",
        "df_limpio = datos[np.array(rutas_adaptadas) != np.array(im_malas)]\n",
        "df_limpio[\"rutas_adap\"] = im_buenas"
      ],
      "metadata": {
        "id": "ZiX2eJbZzHkU"
      },
      "id": "ZiX2eJbZzHkU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extración de caracteristicas.\n",
        "\n",
        "Para esta tarea podría emplearse una red neuronal para obtener un vector de características (embedding). Sin embargo, en este notebook evitaremos este tipo de aproximaciones, ya que no constituyen el objetivo principal del trabajo.\n",
        "\n",
        "En su lugar, utilizaremos HOG (Histogram of Oriented Gradients), un descriptor clásico de visión por computador basado en la distribución de gradientes de intensidad. Este método permite capturar información estructural y de contorno, representando eficazmente las formas presentes en la imagen.\n",
        "\n",
        "Este enfoque resulta especialmente adecuado en el caso de cartas, ya que contienen símbolos y figuras con geometrías bien definidas y altamente distintivas. HOG es particularmente eficaz para modelar este tipo de patrones.\n",
        "\n",
        "Una limitación del descriptor es que opera sobre imágenes en escala de grises, por lo que no aprovecha directamente la información cromática. No obstante, dado que la discriminación entre cartas depende en gran medida de la forma y estructura de los símbolos, es razonable esperar que esta representación sea suficiente para obtener un buen desempeño en la clasificación.\n",
        "\n",
        "Para implentar esto crearemos una función en python que pueda calcular el vector de cada imagen."
      ],
      "metadata": {
        "id": "wbYxUyPCzjws"
      },
      "id": "wbYxUyPCzjws"
    },
    {
      "cell_type": "code",
      "source": [
        "def features_extract(path):\n",
        "  img = cv2.imread(path)\n",
        "  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  features = hog(img_gray,\n",
        "                 pixels_per_cell=(8,8),\n",
        "                 cells_per_block=(2,2),\n",
        "                 feature_vector=True)\n",
        "  return features"
      ],
      "metadata": {
        "id": "OdPpP4h800__"
      },
      "id": "OdPpP4h800__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora obtenemos los vectores de caracteristicas de cada una de las imagenes."
      ],
      "metadata": {
        "id": "jJ27VolY1TqN"
      },
      "id": "jJ27VolY1TqN"
    },
    {
      "cell_type": "code",
      "source": [
        "rutas_train = df_limpio[df_limpio[\"data set\"] == \"train\"].rutas_adap\n",
        "rutas_test = df_limpio[df_limpio[\"data set\"] == \"test\"].rutas_adap\n",
        "rutas_val = df_limpio[df_limpio[\"data set\"] == \"valid\"].rutas_adap\n",
        "\n",
        "X_train = np.array([features_extract(p) for p in rutas_train])\n",
        "X_test = np.array([features_extract(p) for p in rutas_test])\n",
        "X_val = np.array([features_extract(p) for p in rutas_val])"
      ],
      "metadata": {
        "id": "JkeFse6_1bMF"
      },
      "id": "JkeFse6_1bMF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = df_limpio[df_limpio[\"data set\"] == \"train\"][\"labels\"]\n",
        "y_test = df_limpio[df_limpio[\"data set\"] == \"test\"][\"labels\"]\n",
        "y_val = df_limpio[df_limpio[\"data set\"] == \"valid\"][\"labels\"]"
      ],
      "metadata": {
        "id": "3oYs2SqF3_Wx"
      },
      "id": "3oYs2SqF3_Wx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparación de los datos\n",
        "\n",
        "## Normalización de los datos\n",
        "\n",
        "Siempre es necesario normalizar nuestros datos a la hora de entrenar modelo, en nuestro caso usamos `StandardScaler`."
      ],
      "metadata": {
        "id": "TCpWnTzs3P3d"
      },
      "id": "TCpWnTzs3P3d"
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_norm = scaler.fit_transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "X_val_norm = scaler.transform(X_val)"
      ],
      "metadata": {
        "id": "ozc8F2LN4oOl"
      },
      "id": "ozc8F2LN4oOl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Codificación de las etiquetas\n",
        "\n",
        "Usaremos label encoder para que podamos utilizar la regresión logistica, para esta tarea."
      ],
      "metadata": {
        "id": "CdoK6ZNK5EZm"
      },
      "id": "CdoK6ZNK5EZm"
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y_train_enc = encoder.fit_transform(y_train.values.reshape(-1,1))\n",
        "y_test_enc = encoder.transform(y_test.values.reshape(-1,1))\n",
        "y_val_enc = encoder.transform(y_val.values.reshape(-1,1))"
      ],
      "metadata": {
        "id": "a5HavTUj5LVz"
      },
      "id": "a5HavTUj5LVz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento\n",
        "\n",
        "En este apartado entrenaremos el modelo, lo ideal sería aplicar una regresión logistica con CV pero esto al haber muchos datos lo mejor seria entrenar una LR normal"
      ],
      "metadata": {
        "id": "v_hcQxR85lsO"
      },
      "id": "v_hcQxR85lsO"
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(\n",
        "    penalty=\"l2\",\n",
        "    solver=\"lbfgs\",\n",
        "    multi_class=\"multinomial\",\n",
        "    max_iter=2000\n",
        ")\n",
        "model.fit(X_train_norm,y_train_enc)\n"
      ],
      "metadata": {
        "id": "Y1C-6H7P6edO"
      },
      "id": "Y1C-6H7P6edO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}